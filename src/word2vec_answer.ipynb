{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflowを用いたword2vecの実装\n",
    "このハンズオンでは，Tensorflowを用いて単語の埋め込み表現であるword2vecを実装し，学習する．\n",
    "## 目次\n",
    "1. [word2vecの説明](#説明)\n",
    "- [実装](#実装)\n",
    "- [使用例](#使用例)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='説明'></a>\n",
    "## 1. word2vecの説明\n",
    "## 分散表現\n",
    "単語のベクトル表現において，最も単純な方法の１つに単語の辞書を作りonehotのベクトルで表現することができる.<br>\n",
    "しかし全ての単語を独立に扱うと'new'と'novel'は意味が近いといった情報を考慮できない.<br>\n",
    "これに対して分散表現とは，単語間の関係性を埋め込み，単語を低次元のベクトルで表現したものである."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/onehot_dist.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vecとは？\n",
    "単語の分散表現の一種. 類似した文脈で登場する単語は似た意味を持つという仮説に基づく.<br>\n",
    "[参考論文]\n",
    "- Efficient Estimation of Word Representations in Vector Space [Mikolov et al., ICLR workshop, 2013]<br>\n",
    "https://openreview.net/forum?id=idpCdOWtqXd60\n",
    "- Distributed Representations of Words and Phrases and their Compositionality [Mikolov et al., NIPS, 2013]<br>\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vecの学習方法の概要\n",
    "2つのアプローチが提案されている\n",
    "- Continuous Bag-of-Words (CBOW)<br>\n",
    "周辺の単語から単語を予測できるように単語の埋め込みを学習<br>\n",
    "- Skip-Gram<br>\n",
    "周辺の単語を予測できるように単語の埋め込みを学習<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/skip_cbow.png\", width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はSkip-Gramを実装する.\n",
    "モデルは2層の線形変換によって学習する．<br>\n",
    "単語数を$n$，求めるベクトルの次元を$m$とすると，Skip-Gramは以下の図のようになる．<br>\n",
    "ネットワークの入力の単語に対して，周辺単語の出現確率が出力されることを目的として学習する．<br>\n",
    "これによって同じような文脈で登場しやすい単語同士のベクトルが近くなるに学習される\n",
    "<img src=\"figure/neuralnet.png\", width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='実装'></a>\n",
    "## 2. 実装\n",
    "## 2.1 下準備\n",
    "### Tensorflowのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sudo pip install tensorflow-gpu==1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import math\n",
    "import shutil\n",
    "import glob\n",
    "from collections import Counter\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from test_word2vec import ans_make_pair\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの読み込み\n",
    "データセットは今回はptbデータセットというものを用いる.<br>\n",
    "https://github.com/tomsercu/lstm/tree/master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_list = ['data/ptb.train.txt', 'data/ptb.valid.txt', 'data/ptb.test.txt']\n",
    "all_lines = []\n",
    "for file in file_list:\n",
    "    f=open(file)\n",
    "    all_lines.extend(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "data =[]\n",
    "for line in all_lines:\n",
    "    line = line.strip('\\n').lower().split()\n",
    "    data.append(line)\n",
    "print('lines : ', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 前処理\n",
    "1. 頻度の少ない単語を無視して，単語の辞書を作る. 例：{'apple': 0, 'orange': 1, 'banana': 2}<br>\n",
    "<理由>出現頻度の少なすぎる単語はあまり学習できずノイズになる場合が多い.また、単語の辞書が大きくなりすぎると計算量やメモリが膨大になるので頻度に閾値を設ける.<br><br>\n",
    "1. 辞書を用いて文章中の単語をidに変換する\n",
    "1. Skip-Gram用にデータを整形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 頻度が少ない単語は無視して辞書を作成\n",
    "※ (今回用いるptbは出現回数5回未満の単語はunknownに置き換わっているなど、前処理がすでに行われている.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "\n",
    "words = []\n",
    "for sentence in data:\n",
    "    words.extend(sentence)\n",
    "word_cnt = Counter(words)\n",
    "\n",
    "word2id = {'<unk>':0}\n",
    "id2word = {0:'<unk>'}\n",
    "\n",
    "for word, cnt in word_cnt.most_common():\n",
    "    if cnt<min_freq:\n",
    "        break\n",
    "    if word !='<unk>':\n",
    "        word2id[word] = len(word2id)\n",
    "        id2word[len(id2word)] = word\n",
    "    \n",
    "print('vocabulary number is : ', len(word2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 辞書を用いて文章中の単語をidに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "converted_data = []\n",
    "for sentence in data:\n",
    "    id_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2id:\n",
    "            id_sentence.append(word2id[word])\n",
    "        else:\n",
    "            id_sentence.append(word2id['<unk>'])\n",
    "    converted_data.append(id_sentence)\n",
    "    \n",
    "print('<ex>')\n",
    "print('----original sentence---- : ') \n",
    "print(data[2])\n",
    "print('---> id sentence : ')\n",
    "print (converted_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 ターゲットの単語と予測したい周辺単語のペアを作る\n",
    "- 何個隣まで予測するか(window幅について)<br>\n",
    "window幅を大きく取ればより広い文脈を取ることができるが，window幅を大きくしすぎると関係のない文脈まで考慮してしまうというトレードオフがある\n",
    "<img src=\"figure/window.png\", width=200>\n",
    "\n",
    "今回は予めターゲットの単語と予測したい周辺の単語をペアとしてまとめ，訓練時にランダムで読みだすようにする.<br>\n",
    "つまり，'I want to eat an orange'はwindow幅が2の時，以下のようなペアを作る.<br>\n",
    "(I, want), (I, to), (want, I), (want, to), (want, eat), ... , (an, to), (an, eat), (an, orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">TODO</font> make_pair関数を埋めよう\n",
    "- 入力\n",
    "    - 文章のリスト, window幅\n",
    "- 出力\n",
    "    - X_train ... ターゲットの単語のリスト\n",
    "    - y_train ... 予測したい単語のリスト\n",
    "- 例        \n",
    "入力が['I', 'want', 'to', 'eat', 'an', 'orange'], window=2の時，以下のような出力とする.<br>\n",
    "X_train = ['I', 'I', 'want', 'want', 'want', 'to', 'to', 'to', 'to', 'eat', 'eat', 'eat', 'eat', 'an', 'an', 'an', 'orange', 'orange']<br>\n",
    "y_train = ['want', 'to', 'I', 'to', 'eat', 'I', 'want', 'eat', 'an', 'want', 'to', 'an', 'orange', 'to', 'eat', 'orange', 'eat', 'an']<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_pair(sentence, window_size):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    ## TODO\n",
    "    for target_index in range(len(sentence)):\n",
    "        for context_index in range(max(target_index - window_size, 0), min(target_index + window_size+1, len(sentence))): \n",
    "            if target_index != context_index:\n",
    "                X_train.append(sentence[target_index])\n",
    "                y_train.append(sentence[context_index])\n",
    "    ## TODO\n",
    "    return X_train, y_train\n",
    "\n",
    "sentence = ['I', 'want', 'to', 'eat', 'an', 'orange']\n",
    "window = 2\n",
    "\n",
    "x,y = make_pair(sentence, window)\n",
    "print(x)\n",
    "print(y)\n",
    "assert ans_make_pair(sentence, window)==make_pair(sentence, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skip_window = 2 # 何個隣までを予測するか\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for sentence in converted_data:\n",
    "    x, y = make_pair(sentence, skip_window)\n",
    "    X_train.extend(x)\n",
    "    y_train.extend(y)\n",
    "                \n",
    "X_train = np.array(X_train, dtype=np.int32)\n",
    "y_train = np.array(y_train, dtype=np.int32)\n",
    "print('train data : ', len(X_train))\n",
    "print ('<ex> x : ', X_train[0], ' y: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの保存先の指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルの保存先ディレクトリのpath\n",
    "log_path = \"./log/\"\n",
    "if os.path.exists(log_path):\n",
    "    shutil.rmtree(log_path)\n",
    "os.mkdir(log_path)\n",
    "\n",
    "model_path = os.path.join(log_path, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 モデルの実装\n",
    "- 入力部分では単語のidのベクトルを一度線形変換する.これは下の図のような行列演算で，単語のidが$i$の時は線形変換行列の$i$列目を取り出すのと等価である.<br>これは，batchでの処理はtf.nn.embedding_lookup(行列, idのリスト)によって行うことができる.<br>\n",
    "<img src=\"figure/calc.png\", width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 損失関数: Noise Contrastive Estimation (NCE) loss　<br>\n",
    "出力単語の次元が膨大になっても現実的な時間内に計算が終わるように，近似によって計算量を抑える.<br>\n",
    "Softmaxを計算すると計算量が単語の次元数$n$に依存して増えてしまい，計算量が$O(n)$になってしまう．<br>\n",
    "ここで，代わりに負例となる単語$k$個をサンプリングし，ターゲットの周辺に現れる単語と負例となる単語を遠ざけるように学習する.<br>\n",
    "これによって計算量は常に$O(k)$になる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_train = len(X_train)\n",
    "# 単語の種類の数\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "# パラメータ\n",
    "# 学習率(learning rate)\n",
    "lr = 1.0\n",
    "# 学習回数\n",
    "n_epoch = 8\n",
    "# ミニバッチサイズ\n",
    "batch_size = 128\n",
    "# word2vecのベクトルの次元\n",
    "embed_dim = 100\n",
    "# 負例をサンプリングする数\n",
    "num_sampled = 32\n",
    "\n",
    "# 入力\n",
    "x = tf.placeholder(tf.int32, shape=[None])\n",
    "y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "# 単語ごとの埋め込みベクトルの一覧行列．ランダムで初期化する．\n",
    "embed_W = tf.Variable(\n",
    "    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0), name='word_embedding')\n",
    "\n",
    "# 単語のidから埋め込みを取得する．\n",
    "## TODO\n",
    "hidden = tf.nn.embedding_lookup(embed_W, x)\n",
    "## TODO\n",
    "\n",
    "# 埋め込み行列から出力層のネットワークのパラメータ\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocab_size, embed_dim],\n",
    "                        stddev=1.0 / math.sqrt(embed_dim)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "# 損失関数: Noise Contrastive Estimation loss\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights,\n",
    "                   biases=nce_biases,\n",
    "                   inputs=hidden,\n",
    "                   labels=y,\n",
    "                   num_sampled=num_sampled,\n",
    "                   num_classes=vocab_size))\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        sum_loss = 0        \n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "                              \n",
    "        for i in range(0, N_train, batch_size):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batch_size]]\n",
    "            y_batch = y_train[perm[i:i+batch_size]].reshape(-1, 1)\n",
    "        \n",
    "            feed_dict = {x:X_batch, y:y_batch}\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            sum_loss += loss_val * X_batch.shape[0]\n",
    "\n",
    "        print('Train loss %.5f' %(sum_loss/ N_train))\n",
    "        \n",
    "    # 学習されたベクトルの値を取得する\n",
    "    final_embed = embed_W.eval()\n",
    "    \n",
    "    # モデルの保存\n",
    "    saver.save(sess, model_path)\n",
    "    \n",
    "    # 埋め込み空間の可視化用 \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embed_W.name\n",
    "    embedding.metadata_path = 'metadata.tsv'\n",
    "    summary_writer = tf.summary.FileWriter(log_path)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    sorted_dict = sorted(word2id.items(), key=lambda x: x[1])\n",
    "    words = [\"{}\\n\".format(x[0]) for x in sorted_dict]\n",
    "    with open(\"log/metadata.tsv\", \"w\") as f:\n",
    "        f.writelines(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboardを確認してみよう\n",
    "- tensorboard --logdir ./log/ (logのディレクトリを指定)\n",
    "- ブラウザ上でnotebookのhttp://{サーバーのアドレス}:6006/ にアクセスする．\n",
    "- サーバーのアドレスとは，現在用いているjupyter notebookのURLのうち，\"ec2-....compute.amazonaws.com\"で表される形式の文字列．\n",
    "\n",
    "下の画面のように単語のベクトルを可視化したものを確認することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figure/tensorboard.png\", width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='使用例'></a>\n",
    "## 3. 使用例\n",
    "## 類似語検索\n",
    "word2vecによって類似語を検索してみよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ベクトルの正規化\n",
    "norm_embed = final_embed/np.linalg.norm(final_embed, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#　コサイン距離が近い単語上位top_n個取得する\n",
    "def get_sim_word(query, top_n=6):\n",
    "    query = query/np.linalg.norm(query)\n",
    "    cos = 1- np.dot(query[np.newaxis,:], norm_embed.T)[0]\n",
    "    sim = np.argsort(cos)\n",
    "    return cos[sim[:top_n]], sim[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_word = 'president'\n",
    "result = final_embed[word2id[input_word]]\n",
    "scores, indices = get_sim_word(result)\n",
    "for i, index in enumerate(indices):\n",
    "    if id2word[index]!=input_word:\n",
    "        print(i, ' : ', id2word[index], '(distance : {0:.2})'.format(scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
